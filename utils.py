"""Semantic Scholar API é€šç”¨å·¥å…·å‡½æ•°"""
import csv
import time
import requests

BASE_URL = "https://api.semanticscholar.org/graph/v1/paper"


def request_with_retry(url, params=None, max_retries=5):
    """å¸¦é‡è¯•æœºåˆ¶çš„ GET è¯·æ±‚ï¼Œè‡ªåŠ¨å¤„ç† 429 é™æµ"""
    for i in range(max_retries):
        try:
            resp = requests.get(url, params=params)
            if resp.status_code == 200:
                return resp.json()
            if resp.status_code == 429:
                wait = (i + 1) * 3
                print(f"âš ï¸ é™æµä¸­ï¼Œç­‰å¾… {wait}s...")
                time.sleep(wait)
                continue
            return None
        except requests.RequestException as e:
            print(f"è¯·æ±‚å¼‚å¸¸: {e}")
            return None
    print("âŒ é‡è¯•æ¬¡æ•°è€—å°½")
    return None


def search_paper(title):
    """é€šè¿‡æ ‡é¢˜æœç´¢è®ºæ–‡ï¼Œè¿”å› (paper_id, info) æˆ– (None, None)"""
    print(f"ğŸ” æœç´¢: '{title}'")
    data = request_with_retry(
        f"{BASE_URL}/search",
        {"query": title, "limit": 1, "fields": "paperId,title,year"}
    )
    if data and data.get("data"):
        p = data["data"][0]
        return p["paperId"], f"{p['title']} ({p.get('year', 'N/A')})"
    print("âŒ æœªæ‰¾åˆ°")
    return None, None


def fetch_relations(paper_id, relation_type, sort_by="citation"):
    """
    è·å–è®ºæ–‡å…³ç³»æ•°æ®ï¼ˆcitations æˆ– referencesï¼‰
    relation_type: "citations" | "references"
    sort_by: "citation" | "year" | "influential"
    """
    paper_key = "citingPaper" if relation_type == "citations" else "citedPaper"
    fields = f"isInfluential,{paper_key}.title,{paper_key}.year,{paper_key}.citationCount"
    
    print(f"ğŸ“¥ è·å– {relation_type}...")
    results, offset = [], 0
    
    while True:
        data = request_with_retry(
            f"{BASE_URL}/{paper_id}/{relation_type}",
            {"fields": fields, "offset": offset, "limit": 1000}
        )
        if not data or "data" not in data:
            break
        batch = data["data"]
        results.extend(batch)
        if len(batch) < 1000:
            break
        offset += 1000
        print(f"   å·²è·å– {len(results)} æ¡...")
        time.sleep(1)
    
    # æ’åº
    sort_keys = {
        "citation": lambda x: (x.get(paper_key) or {}).get("citationCount") or 0,
        "year": lambda x: (x.get(paper_key) or {}).get("year") or 0,
        "influential": lambda x: (x.get("isInfluential", False), (x.get(paper_key) or {}).get("citationCount") or 0),
    }
    results.sort(key=sort_keys.get(sort_by, sort_keys["citation"]), reverse=True)
    
    print(f"ğŸ“Š å…± {len(results)} æ¡")
    return results, paper_key


def save_to_csv(data, paper_key, output_path):
    """ä¿å­˜ç»“æœåˆ° CSV"""
    with open(output_path, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=["isInfluential", "citationCount", "year", "title"])
        writer.writeheader()
        for item in data:
            p = item.get(paper_key) or {}
            writer.writerow({
                "isInfluential": bool(item.get("isInfluential")),
                "citationCount": p.get("citationCount") or 0,
                "year": p.get("year", "N/A"),
                "title": p.get("title", "Unknown")
            })
    print(f"âœ… å·²ä¿å­˜åˆ° {output_path}")



def safe_filename(title):
    """ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å"""
    return "".join(c if c.isalnum() or c in " -_" else "" for c in title).replace(" ", "_")


def print_results(data, paper_key):
    """æ‰“å°ç»“æœè¡¨æ ¼"""
    if not data:
        print("æ— ç»“æœ")
        return
    
    print(f"\n{'#':<4} {'Year':<6} {'Citations':<10} {'Inf':<4} Title")
    print("-" * 80)
    for i, item in enumerate(data, 1):
        p = item.get(paper_key) or {}
        inf = "âœ“" if item.get("isInfluential") else ""
        print(f"{i:<4} {p.get('year', 'N/A'):<6} {p.get('citationCount') or 0:<10} {inf:<4} {(p.get('title') or 'Unknown')[:50]}")
