"""Semantic Scholar API é€šç”¨å·¥å…·å‡½æ•°"""
import csv
import time
import requests

BASE_URL = "https://api.semanticscholar.org/graph/v1/paper"


def request_with_retry(url, params=None, max_retries=10):
    """å¸¦é‡è¯•æœºåˆ¶çš„ GET è¯·æ±‚ï¼Œè‡ªåŠ¨å¤„ç† 429 é™æµ"""
    for i in range(max_retries):
        try:
            resp = requests.get(url, params=params)
            if resp.status_code == 200:
                return resp.json()
            if resp.status_code == 429:
                wait = (i + 1) * 3
                print(f"âš ï¸ é™æµä¸­ï¼Œç­‰å¾… {wait}s...")
                time.sleep(wait)
                continue
            return None
        except requests.RequestException as e:
            print(f"è¯·æ±‚å¼‚å¸¸: {e}")
            return None
    print("âŒ é‡è¯•æ¬¡æ•°è€—å°½")
    return None


def search_paper(title):
    """é€šè¿‡æ ‡é¢˜æœç´¢è®ºæ–‡ï¼Œè¿”å› (paper_id, info) æˆ– (None, None)"""
    print(f"ğŸ” æœç´¢: '{title}'")
    data = request_with_retry(
        f"{BASE_URL}/search",
        {"query": title, "limit": 1, "fields": "paperId,title,year"}
    )
    if data and data.get("data"):
        p = data["data"][0]
        return p["paperId"], f"{p['title']} ({p.get('year', 'N/A')})"
    print("âŒ æœªæ‰¾åˆ°")
    return None, None


def sort_papers(papers, paper_key, strategy="citation"):
    """
    ç»Ÿä¸€çš„è®ºæ–‡æ’åºé€»è¾‘ (å¤šç»´æ’åºï¼Œstrategy æŒ‡å®šé¦–è¦ç»´åº¦)
    é»˜è®¤ä¼˜å…ˆçº§: citation > influential > year
    strategy: "citation" | "year" | "influential" (æå‡åˆ°ç¬¬ä¸€ä½)
    """
    def key_fn(x):
        p = x.get(paper_key) or {}
        dims = {
            "citation": p.get("citationCount") or 0,
            "influential": bool(x.get("isInfluential")),
            "year": p.get("year") or 0,
        }
        # é»˜è®¤é¡ºåºï¼Œå°† strategy æå‡åˆ°é¦–ä½
        order = ["citation", "influential", "year"]
        if strategy in order:
            order.remove(strategy)
            order.insert(0, strategy)
        return tuple(dims[k] for k in order)
    
    papers.sort(key=key_fn, reverse=True)
    return papers


def fetch_relations(paper_id, relation_type, sort_by="citation", 
                     influential_only=False, since_year=None, until_year=None, 
                     num_results=None, fetch_limit=10000):
    """
    è·å–è®ºæ–‡å…³ç³»æ•°æ®ï¼ˆcitations æˆ– referencesï¼‰
    relation_type: "citations" | "references"
    sort_by: "citation" | "year" | "influential"
    influential_only: æ˜¯å¦åªè¿”å›æœ‰å½±å“åŠ›çš„è®ºæ–‡
    since_year / until_year: å¹´ä»½èŒƒå›´é™åˆ¶ (å«è¾¹ç•Œ)
    num_results: è¿”å›ç»“æœä¸Šé™ (None è¡¨ç¤ºä¸é™åˆ¶)
    fetch_limit: ä» API è·å–çš„æ•°æ®é‡ä¸Šé™ (é»˜è®¤ 10000)
    """
    paper_key = "citingPaper" if relation_type == "citations" else "citedPaper"
    fields = f"isInfluential,{paper_key}.paperId,{paper_key}.title,{paper_key}.year,{paper_key}.citationCount"
    
    print(f"ğŸ“¥ è·å– {relation_type}...")
    results, offset = [], 0
    
    while len(results) < fetch_limit:
        data = request_with_retry(
            f"{BASE_URL}/{paper_id}/{relation_type}",
            {"fields": fields, "offset": offset, "limit": 1000}
        )
        batch = data.get("data") if data else None
        if not batch:
            break
        results.extend(batch)
        if len(batch) < 1000:
            break
        offset += 1000
        print(f"   å·²è·å– {len(results)} æ¡...")
        time.sleep(1)
    
    # è¿‡æ»¤ + æ’åº (filter åœ¨å‰å¯å‡å°‘æ’åºå¼€é”€)
    def passes_filter(x):
        if influential_only and not x.get("isInfluential"):
            return False
        year = (x.get(paper_key) or {}).get("year")
        if since_year and (not year or year < since_year):
            return False
        if until_year and (not year or year > until_year):
            return False
        return True
    
    results = [x for x in results if passes_filter(x)]
    results = sort_papers(results, paper_key, strategy=sort_by)[:num_results]
    
    print(f"ğŸ“Š å…± {len(results)} æ¡")
    return results, paper_key


def save_to_csv(data, paper_key, output_path):
    """ä¿å­˜ç»“æœåˆ° CSV"""
    with open(output_path, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=["isInfluential", "citationCount", "year", "title"])
        writer.writeheader()
        for item in data:
            p = item.get(paper_key) or {}
            writer.writerow({
                "isInfluential": bool(item.get("isInfluential")),
                "citationCount": p.get("citationCount") or 0,
                "year": p.get("year", "N/A"),
                "title": p.get("title", "Unknown")
            })
    print(f"âœ… å·²ä¿å­˜åˆ° {output_path}")


def safe_filename(title):
    """ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å"""
    return "".join(c if c.isalnum() or c in " -_" else "" for c in title).replace(" ", "_")


def print_results(data, paper_key):
    """æ‰“å°ç»“æœè¡¨æ ¼"""
    if not data:
        print("æ— ç»“æœ")
        return
    
    print(f"\n{'#':<4} {'Year':<6} {'Citations':<10} {'Inf':<4} Title")
    print("-" * 80)
    for i, item in enumerate(data, 1):
        p = item.get(paper_key) or {}
        inf = "âœ“" if item.get("isInfluential") else ""
        print(f"{i:<4} {p.get('year', 'N/A'):<6} {p.get('citationCount') or 0:<10} {inf:<4} {(p.get('title') or 'Unknown')[:50]}")
